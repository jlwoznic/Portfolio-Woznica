#  }
new_vec
}
# Create new DF of lesion1
generateLesion1<-function(col)
{
num.rows <- length(col)
colDF <- NULL
colI <-1
while(colI<=num.rows)
{
theVal <- col[colI]
# check length first - don't do for 1 or 5
vec <- as.integer(unlist(str_split(theVal,"")))
numNums <- length(vec)
if (numNums == 4) {colDF <- rbind(colDF, convertLesion1(vec))}
if (numNums == 1) {colDF <- rbind(colDF, c("none", "none", "none", "none"))}
if (numNums == 5) {colDF <- rbind(colDF, c("fix", theVal, "none", "none"))}
if (numNums == 3) {colDF <- rbind(colDF, c("omit", theVal, "none", "none"))}
colI<-colI+1
}
colDF
}
install.packages("stringr")
install.packages("stringr")
# Data loading
colicData <- read.csv("C:/Users/Joyce/Desktop/Syracuse/IST707/Project/Data/horseRaw.csv", header = TRUE, na.strings = "NA")
str(colicData)
# review missing data
tMissing <-sum(is.na(colicData))
cat("The number of missing values in Horse Colic Data is ", tMissing)
# What to do with missing data??
# replace missing data with means - which will not work on rows that are not numeric,
colicDataM <- replaceNAwMeans(colicData)
# now replace with most common factor
colicDataM <- replaceNAwMode(colicDataM)
Hold.colicData <- colicData
colicData<-colicDataM
# review missing data
tMissing <-sum(is.na(colicData))
cat("The number of missing values in Horse Colic Data is ", tMissing)
# do by hand
# nasogastric tube (NA is mode) - must be 'slight'
# abdomo_appearance - must be 'cloudy'
# abdomen - must be 'distend_large"
# rectal_exam_feces - must be 'absent'
colicData$nasogastric_tube[is.na(colicData$nasogastric_tube)] <- "slight"
colicData$abdomo_appearance[is.na(colicData$abdomo_appearance)] <- "cloudy"
colicData$abdomen[is.na(colicData$abdomen)] <- "distend_large"
colicData$rectal_exam_feces[is.na(colicData$rectal_exam_feces)] <- "absent"
# review missing data
tMissing <-sum(is.na(colicData))
cat("The number of missing values in Horse Colic Data is ", tMissing)
# fix the rows that have extensive combinations
# do we just remove these data points since we only have 8 combined for all 300 horses?
length(which(colicData$lesion_3 != 0))
length(which(colicData$lesion_2 != 0))
# drop these columns and eliminate Hospital Number
colicData <- subset(colicData, select=-c(lesion_2, lesion_3, hospital_number))
# separate out the lesion_1 with variables
length(which(colicData$lesion_1 != 0))
# make 3 variables out of lesion_1
# set up the variables
firstL <- c("gastric", "small_intestine", "large_colon", "large_colon_cecum", "cecum", "traverse_colon",
"rectum-descending_colon", "uterus", "bladder", "none", "all_intestinal_sites")
secondL <- c("simple", "strangulation", "inflammation", "other")
secondL.0 <- "none"
thirdL <- c("mechanical", "paralytic")
thirdL.0 <- "none"
fourthL <- c("obturation", "intrinsic", "extrinsic", "adynamic", "volvulous-torsion", "intussuption",
"thromboembolic", "hernia", "lipoma-slenic_incarceration", "displacement")
fourthL.0 <- "none"
# took about 5 hours to figure this out!
new_lesion1Cols <- generateLesion1(colicData$lesion_1)
colnames(new_lesion1Cols) <- c("lesion1A", "lesion1B", "lesion1C", "lesion1D")
new_lesion1Cols <- data.frame(new_lesion1Cols)
# replace colicData$lesion_1 with new 4 rows
colicData <- cbind(colicData, new_lesion1Cols)
colicData <- subset(colicData, select=-c(lesion_1))
# the rows to do manually
# probably a better way, but was in a hurry
fix_manually <- subset(colicData, colicData$lesion1A == "fix")
# fix each manually
fix_seven <- subset(fix_manually, fix_manually$lesion1B == 31110)
fix_seven$lesion1A <- "large_colon"
fix_seven$lesion1B <- "simple"
fix_seven$lesion1C <- "mechanical"
fix_seven$lesion1D <- "displacement"
fix_two <- subset(fix_manually, fix_manually$lesion1B == 11124)
fix_two$lesion1A <- "all_intestinal_sites"
fix_two$lesion1B <- "simple"
fix_two$lesion1C <- "paralytic"
fix_two$lesion1D <- "adynamic"
fix_onea <- subset(fix_manually, fix_manually$lesion1B == 11300)
fix_onea$lesion1A <- "all_intestinal_sites"
fix_onea$lesion1B <- "inflammation"
fix_onea$lesion1C <- "N-A"
fix_onea$lesion1D <- "N-A"
fix_oneb <- subset(fix_manually, fix_manually$lesion1B == 11400)
fix_oneb$lesion1A <- "all_intestinal_sites"
fix_oneb$lesion1B <- "other"
fix_oneb$lesion1C <- "N-A"
fix_oneb$lesion1D <- "N-A"
# need to omit 12208 - not valid value
fix_onec <- subset(fix_manually, fix_manually$lesion1B == 21110)
fix_onec$lesion1A <- "small_insestine"
fix_onec$lesion1B <- "simple"
fix_onec$lesion1C <- "mechanical"
fix_onec$lesion1D <- "displacement"
fix_oned <- subset(fix_manually, fix_manually$lesion1B == 41110)
fix_oned$lesion1A <- "large_colon_cecum"
fix_oned$lesion1B <- "simple"
fix_oned$lesion1C <- "mechanical"
fix_oned$lesion1D <- "displacement"
to_replace <- rbind(fix_seven, fix_two, fix_onea, fix_oneb, fix_onec, fix_oned)
# delete all rows with "fix"
colicData <- subset(colicData, colicData$lesion1A != "fix")
# append fixed rows to colicData
colicData <- rbind(colicData, to_replace)
# maybe use the mode for each column on the incorrect "omit"'s
# remove all rows with omit
wrong_rows <- subset(colicData, colicData$lesion1A == "omit")
# instead of losing these 6 data points, replace with what I THINK the data is
# for 300
fix_300 <- subset(wrong_rows, wrong_rows$lesion1B == 300)
fix_300$lesion1A <- "large_colon"
fix_300$lesion1B <- "N-A" # only value could be
fix_300$lesion1C <- "N-A"
fix_300$lesion1D <- "N-A"
fix_400 <- subset(wrong_rows, wrong_rows$lesion1B == 400)
fix_400$lesion1A <- "large_colon_cecum"
fix_400$lesion1B <- "N-A" # only value could be
fix_400$lesion1C <- "N-A"
fix_400$lesion1D <- "N-A"
to_replace <- rbind(fix_300, fix_400)
colicData <- subset(colicData, colicData$lesion1A != "omit")
colicData <- rbind(colicData, to_replace)
# where capillary_refill_time = 3, change to less_3_sec
fix_caps <- subset(colicData, colicData$capillary_refill_time == 3)
fix_caps$capillary_refill_time <- "less_3_sec"
colicData <- subset(colicData, colicData$capillary_refill_time != 3)
colicData <- rbind(colicData, fix_caps)
View(colicData)
View(colicData)
# ------------------------------------------------------------------
# force no scientific notation
options(scipen=999)
# General Statistics and Analysis
summary(colicData)
genStat <- stat.desc(colicData, basic=F)
View(genStat)
# combine in a plot
table(colicData$outcome)
barplot(table(colicData$outcome), col=rainbow(20))
# overall description (lengthy)
describe(colicData)
test<-as.matrix(colicData)
summary(test)
# descriptive statistics from using stargazer package
stargazer(colicData, type="text")
# ------------------------------------------------------------------
# Initial Visualizations
# boxplot of mucous_membrane
# boxplot of pain level
# abdomen
# histograms of Rectal_temp, pulse, respiratory rate, temp of extremeties, pulse, reflux
hist(colicData$rectal_temp, col=topo.colors(20))
hist(colicData$pulse, col=terrain.colors(20))
hist(colicData$respiratory_rate, col=rainbow(20))
hist(colicData$rectal_temp, col=cm.colors(20))
hist(colicData$total_protein, col=topo.colors(20))
# count these and group by how many of each
# do a which on the values to get totals
# separate data be outcome for plotting reflux
gastroTbl <- table(colicData$outcome, colicData$nasogastric_reflux)
gastroDF <- data.frame(gastroTbl)
colnames(gastroDF)<-c("outcome", "reflux", "horse_count")
barplot(gastroTbl,beside=TRUE, legend=TRUE, col=c("red", "orange", "green"),
main="Horses showing Naso Gastric Reflux by Outcome",
axisnames=TRUE)
# Here we need to remove NA's first***
barplot(table(colicData$outcome, colicData$abdomen), beside=TRUE, legend=TRUE,
col=c("red", "blue", "green"), main="Abdomen Presentation and Outcome",
axisnames=TRUE)
# count these and group by how many of each
# do a which on the values to get totals
# separate data be outcome for rectal exam feces
fecesTbl <- table(colicData$outcome, colicData$rectal_exam_feces)
fecesDF <- data.frame(fecesTbl)
colnames(fecesDF)<-c("outcome", "feces", "horse_count")
barplot(fecesTbl,beside=TRUE, legend=TRUE, col=c("red", "orange", "green"),
main="Number of Horses Feces Status by Colic Outcome",
axisnames=TRUE)
surgTbl <- table(colicData$outcome, colicData$surgery)
barplot(surgTbl,beside=TRUE, legend=TRUE, col=c("red", "orange", "green"),
main="Number of Horses who had Surgery by Colic Outcome",
axisnames=TRUE)
# boxplot of mucous membrane by outcome
# this doesn't make sense - fix it!
# with boxplot
g <- ggplot(colicData, aes(group=mucous_membrane,x=mucous_membrane,y=outcome))
g <- g + geom_boxplot(aes(fill=factor(mucous_membrane)))
g <- g+ theme(axis.text.x = element_text(angle = 45, hjust = 1))
g <- g + ggtitle("Outcome by Mucous Membrane") + theme(plot.title=element_text(hjust=0.5))
g
# Some initial plotting of the data to see what we have
plot(colicData$outcome,colicData$pulse)
# boxplot of mucous membrane by outcome
# with boxplot
g <- ggplot(colicData, aes(group=outcome,x=outcome,y=pulse))
g <- g + geom_boxplot(aes(fill=factor(outcome)))
g <- g+ theme(axis.text.x = element_text(angle = 45, hjust = 1))
g <- g + xlab("Outcome") + ylab("Pulse")
g <- g + ggtitle("Pulse Rate by Outcome") + theme(plot.title=element_text(hjust=0.5))
g
# Plot pulse rate by outcome
g <- ggplot(colicData, aes(x=outcome, y=pulse)) + geom_point()
g <- g + stat_smooth(method= "lm", col=rainbox(20))
g <- g + ggtitle("Pulse Rates by Outcome")
g <- g + xlab("Outcome") + ylab("Pulse")
g
# Are men or women more satisfied?
# Reflected in Figure 9
g<- ggplot(colicData, aes(x=age, fill=factor(outcome)))
g<- g+ ggtitle("Outcome by Age")
g<- g+ xlab("Age") + ylab("Outcome")
g<- g + geom_bar(position="dodge")
g
# Look at some of the data via a scatter plot
g <- ggplot(colicData, aes(x=temp_of_extremities, y=respiratory_rate))
g <- g + geom_point(aes(color=outcome, size=age))
g <- g + ggtitle("Outcome using Age, Respiratory Rate and Extremity Temperature")
g <- g + theme(plot.title=element_text(hjust=0.5))
g <- g + xlab("Extremity Temperature") + ylab("Respiratory Rate")
g
X <- colicData
## k means visualization results!
distance1 <- get_dist(X,method = "manhattan")
fviz_dist(distance1, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
distance2 <- get_dist(X,method = "euclidean")
fviz_dist(distance2, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
distance3 <- get_dist(X,method = "spearman")
fviz_dist(distance3, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07", title= "Distance Matrix Visualization, Spearman Method"))
str(X)
# ------------------------------------------------------------------
# Decision Tree
##Make Train and Test sets
trainRatio <- .60
set.seed(11) # Set Seed so that same sample can be reproduced in future also
sample <- sample.int(n = nrow(colicData), size = floor(trainRatio*nrow(colicData)), replace = FALSE)
train <- colicData[sample, ]
test <- colicData[-sample, ]
# train / test ratio
length(sample)/nrow(colicData)
##--------------------------------------------------------------------------------------------------------
#Decision Tree Models
#Train Tree Model 1
install.packages("rpart")
require(rpart)
library(rpart)
require(rpart.plot)
require(e1071)
install.packages("rattle")
library(rattle)
require(rattle)
# start with first Training Tree 1
train_tree1 <- rpart(outcome ~ ., data = train, method="class", control=rpart.control(cp=0))
# start with first Training Tree 1
train_tree1 <- rpart(outcome ~ ., data = train, method="class", control=rpart.control(cp=0))
summary(train_tree1)
#predict the test dataset using the model for train tree No. 1
predicted1 <- predict(train_tree1, test, type="class")
#plot number of splits
rsq.rpart(train_tree1)
#predict the test dataset using the model for train tree No. 1
predicted1 <- predict(train_tree1, test, type="class")
#plot number of splits
rsq.rpart(train_tree1)
plotcp(train_tree1)
#plot the decision tree
fancyRpartPlot(train_tree1, cex=0.8, caption="Training Tree 1")
#confusion matrix to find correct and incorrect predictions
table(Outcome=predicted1, true=test$outcome)
#Train Tree Model 2
train_tree2 <- rpart(outcome ~ ., data = train, method="class", control=rpart.control(cp=0, minsplit = 2, maxdepth = 5))
summary(train_tree2)
#predict the test dataset using the model for train tree No. 2
predicted2<- predict(train_tree2, test, type="class")
#plot number of splits
rsq.rpart(train_tree2)
#plot the decision tree
fancyRpartPlot(train_tree2, cex=0.7, caption="Training Tree 2")
#confusion matrix to find correct and incorrect predictions
table(Outcome=predicted2, true=test$outcome)
#Train Tree Model 3
train_tree3 <- rpart(outcome ~ ., data = train, method="class", control=rpart.control(cp=0, minsplit = 5, maxdepth = 5))
summary(train_tree3)
#predict the test dataset using the model for train tree No. 3
predicted3 <- predict(train_tree3, test, type="class")
#plot number of splits
rsq.rpart(train_tree3)
#plot the decision tree
fancyRpartPlot(train_tree3, cex=0.7, caption="Training Tree 3")
#confusion matrix to find correct and incorrect predictions
table(Outcome=predicted3, true=test$outcome)
#Train Tree Model 4 ** BEST MODEL **
train_tree4 <- rpart(outcome ~ ., data = train, method="class", control=rpart.control(cp=0, minsplit = 3, maxdepth = 5))
summary(train_tree4)
#predict the test dataset using the model for train tree No. 4
predicted4 <- predict(train_tree4, test, type="class")
#plot number of splits
rsq.rpart(train_tree4)
#plot the decision tree
fancyRpartPlot(train_tree4, cex=0.7, caption="Training Tree 4")
#confusion matrix to find correct and incorrect predictions
table(Outcome=predicted4, true=test$outcome)
#Train Tree Model 5 ** BEST MODEL **
train_tree5 <- rpart(outcome ~ ., data = train, method="class", control=rpart.control(cp=0, minsplit = 3, maxdepth = 4))
summary(train_tree5)
#predict the test dataset using the model for train tree No. 5
predicted5 <- predict(train_tree5, test, type="class")
#plot number of splits
rsq.rpart(train_tree5)
#plot the decision tree
fancyRpartPlot(train_tree5, cex=0.7, caption="Training Tree 5")
#confusion matrix to find correct and incorrect predictions
table(Outcome=predicted5, true=test$outcome)
#Train Tree Model 6 ** BEST MODEL **
train_tree6 <- rpart(outcome ~ ., data = train, method="class", control=rpart.control(cp=0, minsplit = 3, maxdepth = 3))
summary(train_tree6)
#predict the test dataset using the model for train tree No. 6
predicted6 <- predict(train_tree6, test, type="class")
#plot number of splits
rsq.rpart(train_tree6)
#plot the decision tree
fancyRpartPlot(train_tree6, cex=0.7, caption="Training Tree 6")
#confusion matrix to find correct and incorrect predictions
table(Outcome=predicted6, true=test$outcome)
table(Outcome=predicted1, true=test$outcome)
table(Outcome=predicted2, true=test$outcome)
table(Outcome=predicted3, true=test$outcome)
table(Outcome=predicted4, true=test$outcome)
table(Outcome=predicted5, true=test$outcome)
##--------------------------------------------------------------------------------------------------------
# Now let's use the best tree (#1) to predict the disputed papers
ver_pred1 <- predict(train_tree1, newdata=Verification_Papers)
##--------------------------------------------------------------------------------------------------------
# Now let's use the best tree (#1) to predict the disputed papers
# need to find a test set
ver_pred1 <- predict(train_tree1, newdata=test)
ver_pred1
##--------------------------------------------------------------------------------------------------------
# Now let's use the best tree (#2) to predict
ver_pred2 <- predict(train_tree2, newdata=test)
ver_pred2
##--------------------------------------------------------------------------------------------------------
# Now let's use the best tree (#3) to predict
ver_pred <- predict(train_tree3, newdata=test)
ver_pred
# Pruning the tree
printcp(train_tree3)
plotcp(train_tree3)
ptrain_tree3 <- prune(train_tree3, cp = train_tree3$cptable[which.min(train_tree3$cptable[,"xerror"]), "CP"])
fancyRpartPlot(ptrain_tree3, uniform=TRUE, main="Pruned Classification Tree")
# Pruning the tree - find the best model (4, 5 or 6)
printcp(train_tree6)
plotcp(train_tree6)
ptrain_tree6 <- prune(train_tree6, cp = train_tree6$cptable[which.min(train_tree6$cptable[,"xerror"]), "CP"])
fancyRpartPlot(ptrain_tree6, uniform=TRUE, main="Pruned Classification Tree 6")
##--------------------------------------------------------------------------------------------------------
# Trying some Clustering
#m <- [1:2, 1:3]
colicData.Matrix <- as.matrix(colicData)
distMatrix_E <- dist(m, method="euclidean")
#print(distMatrix_E)
distMatrix_M <- dist(m, method="manhattan")
#print(distMatrix_M)
distMatrix_C <- dist(m, method="cosine")
#print(distMatrix_C)
distMatrix_C_norm <- dist(m_norm, method="cosine")
##Clustering
##Below are some HAC results. Which does best??? Why?
###Clustering Methods:
## HAC: Hierarchical Algorithm Clustering Method
## Euclidean
groups_E <- hclust(distMatrix_E,method="ward.D")
plot(groups_E, cex=0.5, font=22, hang=-1, main = "HAC Cluster Dendrogram with Euclidean Similarity")
rect.hclust(groups_E, k=2)
library(stats)
##--------------------------------------------------------------------------------------------------------
# Trying some Clustering
install.packages ("stats")
install.packages("stats")
library(stats)
colicData.Matrix <- as.matrix(colicData)
distMatrix_E <- dist(m, method="euclidean")
m <- colicData.Matrix
distMatrix_E <- dist(m, method="euclidean")
#print(distMatrix_E)
distMatrix_M <- dist(m, method="manhattan")
#print(distMatrix_M)
distMatrix_C <- dist(m, method="cosine")
#print(distMatrix_C)
distMatrix_C_norm <- dist(m_norm, method="cosine")
##Clustering
##Below are some HAC results. Which does best??? Why?
###Clustering Methods:
## HAC: Hierarchical Algorithm Clustering Method
## Euclidean
groups_E <- hclust(distMatrix_E,method="ward.D")
plot(groups_E, cex=0.5, font=22, hang=-1, main = "HAC Cluster Dendrogram with Euclidean Similarity")
rect.hclust(groups_E, k=2)
# Clustering with Manhattan
groups_M <- hclust(distMatrix_M,method="ward.D")
plot(groups_M, cex=0.5, font=22, hang=-1, main = "HAC Cluster Dendrogram with Manhattan Similarity")
rect.hclust(groups_M, k=2)
## Cosine Similarity
groups_C <- hclust(distMatrix_C,method="ward.D")
colicData.Matrix <- as.matrix(colicData)
m <- colicData.Matrix
distMatrix_E <- dist(m, method="euclidean")
#print(distMatrix_E)
distMatrix_M <- dist(m, method="manhattan")
#print(distMatrix_M)
distMatrix_C <- dist(m, method="cosine")
rect.hclust(groups_M, k=2)
# Determine number of clusters
wss <- (nrow(X)-1)*sum(apply(X,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(X,
centers=i)$withinss)
##Clustering
##Below are some HAC results. Which does best??? Why?
###Clustering Methods:
## HAC: Hierarchical Algorithm Clustering Method
## Euclidean
groups_E <- hclust(distMatrix_E,method="ward.D")
plot(groups_E, cex=0.5, font=22, hang=-1, main = "HAC Cluster Dendrogram with Euclidean Similarity")
rect.hclust(groups_E, k=2)
# Clustering with Manhattan
groups_M <- hclust(distMatrix_M,method="ward.D")
plot(groups_M, cex=0.5, font=22, hang=-1, main = "HAC Cluster Dendrogram with Manhattan Similarity")
rect.hclust(groups_M, k=2)
# Determine number of clusters
wss <- (nrow(X)-1)*sum(apply(X,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(X,
centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
# K-Means Cluster Analysis
fit <- kmeans(X, 5) # 5 cluster solution
# get cluster means
aggregate(X,by=list(fit$cluster),FUN=mean)
# append cluster assignment
mydata <- data.frame(X, fit$cluster)
## k means clustering Methods
X <- m_norm
k2 <- kmeans(X, centers = 2, nstart = 100, iter.max = 50)
# K-Means Cluster Analysis
fit <- kmeans(X, 5) # 5 cluster solution
# get cluster means
aggregate(X,by=list(fit$cluster),FUN=mean)
# K-Means Cluster Analysis
fit <- kmeans(X, 5) # 5 cluster solution
# get cluster means
aggregate(X,by=list(fit$cluster),FUN=mean)
# K-Means Cluster Analysis
fit <- kmeans(X, 5) # 5 cluster solution
myARM <- function (df, conf_int, supp_num, min_length, max_length)
{
apriori(df, parameter=list(supp=supp_num, conf=conf_int, minlen=min_length, maxlen=max_length),
appearance = list(default="lhs", rhs="outcome=lived"),
control = list(verbose=F))
}
myRules <- myARM(colicData, 0.9, 0.001, 2, 4)
myRules <- myARM(colicData, 0.9, 0.07, 2, 4)
View(trimCity)
summary(colicData$nasogastric_reflux_ph)
# Create bins for the nasogastric_reflux_ph
colicDataARM <- colicData
colicDataARM$nasogastric_reflux_ph <- cut(colicDataARM$nasogastric_reflux_ph, breaks = c(0, 1.5, 3, 4.5, 6, 7.5),
labels=c("< 1.5", "1.5 to 3", "3 to 4.5", "4.5 to 6", "6 to 7.5"))
View(colicDataARM)
View(colicDataARM)
myRules <- myARM(colicData, 0.9, 0.001, 2, 4)
myRules <- myARM(colicDataARM, 0.9, 0.001, 2, 4)
summary(colicData$abdomo_protein)
# Create bins for the nasogastric_reflux_ph
colicDataARM <- colicData
colicDataARM$nasogastric_reflux_ph <- cut(colicDataARM$nasogastric_reflux_ph, breaks = c(0, 1.5, 3, 4.5, 6, 7.5),
labels=c("< 1.5", "1.5 to 3", "3 to 4.5", "4.5 to 6", "6 to 7.5", "over 7.5"))
colicDataARM$nasogastric_reflux_ph <- cut(colicDataARM$nasogastric_reflux_ph, breaks = c(0, 1.5, 3, 4.5, 6, 7.5),
labels=c("< 1.5", "1.5 to 3", "3 to 4.5", "4.5 to 6", "6 to 7.5"))
# repeat for abdomo_protein
colicDataARM$abdomo_protein <- cut(colicDataARM$abdomo_protein, breaks = c(0, 1.5, 3, 4.5, 6, 7.5, 9, 10.5),
labels=c("< 1.5", "1.5 to 3", "3 to 4.5", "4.5 to 6", "6 to 7.5", "7.5 to 9", "9 to 10.5"))
myRules <- myARM(colicDataARM, 0.9, 0.001, 2, 4)
inspect(myRules)
# inspect the rules
inspect(myRules[1:25])
## sorted
SortedmyRules_conf <- sort(myRules, by="confidence", decreasing=TRUE)
inspect(SortedmyRules_conf[1:20])
SortedmyRules_sup <- sort(myRules, by="support", decreasing=TRUE)
inspect(SortedmyRules_sup[1:20])
SortedmyRules_lift <- sort(myRules, by="lift", decreasing=TRUE)
inspect(SortedmyRules_lift)
# ----------------------------------------------------------------------------------------------------------------------
# Visualize the Rules
plot (SortedmyRules_sup[1:10],method="graph",interactive=TRUE,shading="confidence")
plot (SortedmyRules_conf[1:10],method="graph",interactive=TRUE,shading="confidence")
arulesViz::plotly_arules(SortedmyRules_conf[1:30])
arulesViz::plotly_arules(SortedmyRules_sup[1:30])
